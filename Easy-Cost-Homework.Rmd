---
title: "R Notebook"
output: html_notebook
---

**Libraries**

We load the packages relevant for the exercise. 

```{r}

library(FactoMineR)
library(tidyr)
library(dplyr)
library(tidyverse)
library(magrittr)
library(ggplot2)
library(ggpubr)
library(factoextra)
library(gridExtra)
library(moments)

```


**Screw Caps Data**

The data ScrewCap.csv contains 195 lots of screw caps described by 11 variables. Diameter, weight, length are the physical characteristics of the cap; nb.of.pieces corresponds to the number of elements of the cap (the picture above corresponds to a cap with 2 pieces: the valve (clapet) is made of a different material); Mature.volume corresponds to the number of caps ordered and bought by the company (number in the lot).

```{r}

raw_data <- read.table("ScrewCaps.csv",header=TRUE, sep=",", dec=".", row.names=1)
head(raw_data)
dim(raw_data)
summary(raw_data)
```

**2) We start with univariate and bivariate descriptive statistics. Using appropriate plot(s) or summaries answer the following questions.**

*a) How is the distribution of the Price? Comment your plot with respect to the quartiles of the Price.*

From the quantile data, the summary statistics are given by: median, 1Q and 3Q  as 14.432, 11.864 and 19.04 respectively. 

The plots, the kurtosis and the skewness parameters suggest the price follows a bimodal distribution that is "skewed right". 

The major mode is around 14 and the antimode is around 29. Furthermore, 50% of the prices in the range 11.864 and 19.04. This is consistent with graph where the majority of the density is concentrated inside this range and a long right tail of prices outside. 

The boxplot supports this analyis and suggests the values in the tail are outliers. 

```{r}

price_density <- ggdensity(raw_data,x="Price",y = "..count..",
                        color="darkblue",
                        fill="lightblue",size=0.5, 
                        alpha=0.2, 
                        title = "Screw Cap Price Distribution", 
                        linetype = "solid", add = c("median"))+ font("title", size = 12,face="bold")
  

  

price_boxplot <- ggboxplot(raw_data$Price, width = 0.1, fill ="lightgray", outlier.colour = "darkblue", outlier.shape=4.2, ylab = "Price", xlab = "Screw Caps" , title = "Price Box Plot") + rotate() + font("title", size = 12,face="bold")



price_quantile <- quantile(raw_data$Price)

ggarrange(price_density, price_boxplot, ncol = 1, nrow = 2)

price_quantile
skewness(raw_data$Price)
kurtosis(raw_data$Price)


```



*b) Does the Price depend on the Length? weight?*

We examine Price vs. Length, log(Price) vs. log(Length); Price vs. weight, log(Price) vs. log(weight) and provide the summary for each. 

The plots suggest somewhat of a relationship between the variables, but observing the R-sqaured values and the results of the F and T tests confirm this to a high degree of significance. 

```{r}
price_length <- ggplot(raw_data, aes(x=Length, y=Price)) + geom_point() + geom_smooth(method=lm, color="darkgreen")+ theme_minimal()
price_length_log <- ggplot(raw_data, aes(x=log(Length), y=log(Price))) + geom_point() + geom_smooth(method=lm, color="darkgreen")+ theme_minimal()
price_weight <- ggplot(raw_data, aes(x=weight, y=Price)) + geom_point() + geom_smooth(method=lm,color="red")+theme_minimal()
price_weight_log <- ggplot(raw_data, aes(x=log(weight), y=log(Price))) + geom_point() + geom_smooth(method=lm,color="red")+theme_minimal()

ggarrange(ggarrange(price_length, price_length_log, ncol = 2, nrow = 1), ggarrange(price_weight, price_weight_log, ncol = 2, nrow = 1), ncol = 1, nrow = 2)

summary(lm(formula = Price ~ Length, raw_data))
summary(lm(formula = log(Price) ~ log(Length), raw_data))
summary(lm(formula = Price ~ weight, raw_data))
summary(lm(formula = log(Price) ~ log(weight), raw_data))
```


*c) Does the Price depend on the Impermeability? Shape?*  

Concerning Impermeability, the plots below show that there are some striking differences between the price distribution for Type 1 and Type 2, in particular observing the medians and the IQR. 

Concerning Shapes, it is difficult to make any real conclusions regarding shape 3 and shape 4 given there are so few data points. We turn our attention to Shape 1 and Shape 2 - the IQR for these two shapes are seemingly different. This is confirmed by the result of the T Test. 


```{r}
impermability_plot_1 <- ggdotplot(raw_data,x="Impermeability",y="Price",color = "Impermeability", palette = "jco",binwidth = 1,legend="none")
shape_plot_1 <- ggdotplot(raw_data,x="Shape",y="Price",color = "Shape", palette = "npg",binwidth = 1,legend="none")
impermability_plot_2 <- ggboxplot(raw_data,x="Impermeability",y="Price",color = "Impermeability", palette = "jco",legend="none")
shape_plot_2 <- ggboxplot(raw_data,x="Shape",y="Price",color = "Shape", palette = "npg", legend = "none")


ggarrange(ggarrange(impermability_plot_1,impermability_plot_2,ncol = 2, nrow = 1),
           ggarrange(shape_plot_1,shape_plot_2,ncol = 2, nrow = 1),
           ncol = 1, nrow = 2)

summary(lm(Price~ Impermeability, data=raw_data))
summary(lm(Price~ Shape, data=raw_data))

```

*d) Which is the less expensive Supplier?*

The answer to this question depends on the definition of expensive. 

First, examine the following absolute metrics (this can be seen via the boxplot)
1) Absolute price - Supplier B cheapest (6.477451). However, Supplier B is also the supplier which has the highest absolute price (46.610372)
2) Average Price - Supplier C cheapest (14.88869)

Second, examine the following relative metrics:  
3) Average Price / Unit Length - Supplier A (1.505043)
4) Average Price / Unit weight - Supplier A (9.013902)
5) Average Price / Unit Diameter - Supplier A (11.95632)
6) Average Price / Unit Mature.Volume - Supplier B (1.663305)

The result above suggest Supplier A has the cheapest average price per unit of production.

The analysis however is not complete given we do not have a definition of cheapest price. Even the scatter and box plots below suggest suppliers may cater to specific product ranges. We also have little data for Supplier C. We have not performed  statistical tests to examine the significance of these differences. 

Furthermore, the analysis ignores the categorical data which could provide some insights into cheapest price for certain product features. 

```{r}

supplier_plot_1 <- ggboxplot(raw_data,x="Supplier",y="Price",color = "Supplier", palette = c("darkblue","red","darkgreen"),legend="none") + rotate()
supplier_plot_2 <- ggscatter(raw_data,x="Length",y="Price",color = "Supplier", palette = c("darkblue","red","darkgreen"),xscale= "log10", yscale="log10")
supplier_plot_3 <- ggscatter(raw_data,x="weight",y="Price",color = "Supplier", palette = c("darkblue","red","darkgreen"),xscale= "log10", yscale="log10")
supplier_plot_4 <- ggscatter(raw_data,x="Diameter",y="Price",color = "Supplier", palette = c("darkblue","red","darkgreen"),xscale= "log10", yscale="log10")

supplier_statistics <- raw_data %>% group_by(Supplier) %>%  summarise( "Average Price" = mean(Price), "Average Length" = mean(Length),"Average weight" = mean(weight),"Average Diameter" = mean(Diameter),"Average Mature.Volume" = mean(Mature.Volume)  ,"Average Price / Length" = mean(Price)/mean(Length), "Average Price / weight" = mean(Price)/mean(weight), "Average Price / Diameter" = mean(Price)/mean(Diameter),"Average Price / Mature.Volume" = mean(Price)/mean(Mature.Volume))

supplier_plot_1
supplier_plot_2
supplier_plot_3
supplier_plot_4
head(supplier_statistics)

```



**3) One important point in explanatory data analysis consists in identifying potential outliers. Could you give points which are suspect regarding the Mature.Volume variable? Give the characteristics (other features) of the observations that seem suspsect**

There are four data points which seem suspect - they have the same characteristics for Diameter, weight, nb.of.pieces, Impermeability, Finishing, Raw.Material and Mature.Volume. They differ in their supplier, price and length. These suggest some error in collating the data (system error / default data). 

```{r}

Mature.Volume_plot <- gghistogram(raw_data,x="Mature.Volume",y="..count..", color = "darkblue", fill = "lightgrey") + theme_minimal()
Mature.Volume_plot
raw_data %>% filter (Mature.Volume > 6e+05 ) 

```


For the rest of the analysis, the 4 data points above are disregarded.

```{r}
library(dplyr)
raw_data <- raw_data %>% filter (Mature.Volume < 6e+05 )
```


We will quickly check that there are no other noticeable outliers - this is indeed the case.  
 
```{r}

check_1 <- gghistogram(raw_data,x="Length",y="..count..", color = "darkblue", fill = "lightgrey") + theme_minimal()
check_2 <- gghistogram(raw_data,x="Diameter",y="..count..", color = "darkblue", fill = "lightgrey") + theme_minimal()
check_3 <- gghistogram(raw_data,x="weight",y="..count..", color = "darkblue", fill = "lightgrey") + theme_minimal()
check_4 <- gghistogram(raw_data,x="nb.of.pieces",y="..count..", color = "darkblue", fill = "lightgrey",bins=40) + theme_minimal()

ggarrange(ggarrange(check_1,check_2,ncol=2,nrow=1),ggarrange(check_3,check_4,ncol=2,nrow=1),ncol=1,nrow=2)

```


**4) Perform a PCA on the dataset ScrewCap, explain briefly what are the aims of a PCA and how categorical variables are handled?**

Principal components analysis (PCA) is a technique for taking high-dimensional data, and using the dependencies between the variables to represent it in a more tractable, lower-dimensional form, without losing too much information - we try capture the essence of high dimentional data in a low dimensional representation. The aim of PCA is to draw conclusions from the linear relationships between variables by detecting the principal dimensions of variability. This may be for compression, denoising, data completion, anomaly detection or for preprocessing before supervised learning (improve performance / regularization to reduce overfitting).

The categorical variables cannot be represented in the same way as the supplementary quantitative variables since it is not possible to calculate the correlation between a categorical variable and the principal components. The categorical variables here are handled as supplemetary variables on a purely illustrative basis  - they are not used to calculate the distance between inidividuals. We represent a categorical variable at the barycentre of all the individuals possessing that variable. A categorical variable on the PCA performed below can therefore be regarded as the mean individual obtained from the set of individuals who have it.

Given our ultimate goal here is to explore data prior to a multiple regression, it is advisable to choose the explanatory variables for the regression model as active variables for PCA, and to project the variable to be explained (the dependent variable) as a supplementary variable. This gives some idea of the relationships between explanatory variables and thus of the need to select explanatory variables. This also gives us an idea of the quality of the regression: if the dependent variable is appropriately projected, it will be a well-fitted model. Thus we select Price as a supplementary variable. 

The dataset in this exercise contains 6 supplementary variables:
- 1 quantitative variable (Price)
- 5 qualitative variables (Supplier, Shape, Impermeability and Finishing).  


```{r}

res.pca <- PCA(raw_data,quali.sup = c(1,5,6,7,9),quanti.sup = 10, graph = FALSE)

fviz_pca_ind(res.pca, col.ind="cos2", label=c("quali"), geom = "point", title = "Individual factor map (PCA)", habillage = "none") + scale_color_gradient2(low="lightblue", mid="blue", high="darkblue", midpoint=0.6) + theme_minimal() 
plot.PCA(res.pca,choix = c("ind"),invisible = c("ind"))+theme_minimal()
plot.PCA(res.pca,choix = c("var"))+theme_minimal()
```



**5) Compute the correlation matrix between the variables and comment it with respect to the correlation circle**

The first task is to center and standardize the variables. Then the correlation matrix is computed. All variable vectors are quite near to the boundary of the correlation circle on the variables plot - thus the variables are relatively well projected on the 2 dimensional subspace. We now turn our attention to correlations between variables. 
The correlations can be visualised through the angles between variables on the correlation matrix. This can be related to the correlation matrix (small angles suggest large positive correlation, 90 degree angles suggest no correlation, 180 degree angles suggest large negative correlation).
- Diameter, Length and weight expose very strong corrleation: the angle between them is close to 0, suggesting correlation close to 1. These are all very highly correlated to the first Principal Component.
- The three variables above are at an angle sightly wider than a right angle to both nb.of.pieces and Mature.Volume in the cirlce which suggests slightly negative correlation.
- Price is highlighly correlatd to the three variables above
- Equally, Mature.Volume and nb.of.pieces are at a slightly wider angle than a right angle which suggests slightly negative correlation - this suggests that when the screw caps have a high number of pieces, the company orders a smaller volume of these. These are well projected on the second principal component.

```{r}
don <- as.matrix(raw_data[,-c(1,5,6,7,9,10)]) %>% scale()
don_correlation <- cor(don)
don_correlation
plot.PCA(res.pca,choix = c("var"))+theme_minimal()
```

**6) On what kind of relationship PCA focuses? Is it a problem?**

PCA focuses on the linear relationships between continuous variables. Given complex links also exist, such as quadratic relationships, logarithmics, exponential functions, and so forth, this may seem restrictive, but in practice many relationships can be considered linear, at least for an initial approximation. However, there is obviously non-linear datasets for which PCA will have pitfalls (e.g. spiral dataset). Furthermore, in PCA categorical variables cannot be active variables, which can be restrictive. 
**7) Comment the PCA outputs**

*Comment the position of the categories Impermeability=type 2 and Raw.Material=PS.*

The coordinates for Type 2 are (3.30430162 , 0.0020023422) for the first two principal components. It has a very significant p value for Dim 1 -  the coordinate issignificantly different from 0 on the first component. 
The coordinate for PS are (2.69084507 -0.2539199538) for the first two principal components. It has a very significant p value for Dim 1-  the coordinate issignificantly different from 0 on the first component. 

Furthermore, given the correlation circle shows high correlation between the first component and price, diameter, length and weight, this suggest Type 2 and PS have high values for these variables. 


```{r}
res.pca$quali.sup$coord
dimdesc(res.pca, 1:2)
```

*Comment the percentage of inertia*

Below in the Scree plot we see the percentage of inertia explained by each component. Over 95% of the variance can be explained with the 3 first synthetic vectors in the PCA. Furthermore, we can see that the variance of the first component is explained in majority by Diameter, Length and weight as expected. In the second and third dimension by nb.of.pieces and Mature.Volume. 

```{r}
res.pca$eig
fviz_eig(res.pca, addlabels = TRUE)
res.pca$var$contrib
```



**8) Give the R object with the two principal components which are the synthetic variables the most correlated to all the variables.**

These are found in the code below - 

```{r}
res.pca$var$coord[,1:2]
```

**9) PCA is often used as a pre-processing step before applying a clustering algorithm, explain the rationale of this approach and how many components k you keep.**

We chose the maximum number of components as to not to lose any significant information whilst discarding the components that can be considered as noise. Consequently, we keep the number of dimensions such that we keep 95% of the inertia in PCA, which is equivalent to 3 components in our analysis. 

**10) Perfoms a kmeans algorithm on the selected k principal components of PCA. How many cluster are you keeping? Justify.**

We us the Elbow method and look at the knee to determine the number of clusters we keep 3 clusters here. 

```{r}
#Perform PCA on 3 Principal Components
res.pca_3 <- PCA(raw_data,quali.sup = c(1,5,6,7,9),quanti.sup = 10, ncp=3, graph = FALSE) 

#Use the Elbow method to determine the number of clusters to keep
fviz_nbclust(res.pca_3$ind$coord, kmeans, method = "wss") + geom_vline(xintercept = 3, linetype = 2)

#Kmeans Algorithm 
k_cluster <- kmeans(res.pca_3$ind$coord, 3)
k_mean_data <- as.matrix(res.pca_3$ind$coord)
plot(k_mean_data, col = k_cluster$cluster, pch = 20, frame = FALSE,  main = "K-means")

```

**11) Perform an AHC on the selected k principal components of PCA.**

Below we perform an AHB on the select 3 principal components of PCA. 

```{r}

res.pca_3 <- PCA(raw_data,quali.sup = c(1,5,6,7,9),quanti.sup = 10, ncp=3, graph = FALSE) 

res.hcpc <- HCPC(res.pca_3, nb.clust = -1, graph = TRUE)

plot.HCPC(res.hcpc, choice = 'map', draw.tree = FALSE, select = "cos2 5", title = '')
plot.HCPC(res.hcpc, choice = 'bar', draw.tree = FALSE, title = 'Inertia Gain')
plot.HCPC(res.hcpc, choice = '3D.map', draw.tree = FALSE, title = '', angle=60)

summary(res.hcpc)

```


**12) Comments the results and describe precisely one cluster.**

From the FactoInvestigate package, we get the basic analysis below:

The cluster 1 is made of individuals sharing :4
- high values for the variable Mature.Volume. 
- low values for the variables nb.of.pieces, Price, weight, Length and Diameter (variables are sorted from the weakest).

The cluster 2 is made of individuals sharing :
- high values for the variable nb.of.pieces. 
- low values for the variables Mature.Volume, Diameter, Length, weight and Price (variables are sorted from the weakest).

The cluster 3 is made of individuals such as 89, 90, 131, 161, 163 and 164. This group is characterized by :
- high values for the variables Length, Diameter, weight and Price (variables are sorted from the strongest).
- low values for the variables nb.of.pieces and Mature.Volume (variables are sorted from the weakest).

```{r}
res.hcpc$desc.var
```





*If someone ask you why you have selected k components and not k + 1 or k − 1, what is your answer? (could you suggest a strategy to assess the stability of the approach? - are there many differences between the clustering obtained on k components or on the initial data)*

```{r}
res.pca <- PCA(raw_data,quali.sup = c(1,5,6,7,9),quanti.sup = 10,ncp=4)
res.hcpc <- HCPC(res.pca, nb.clust = -1)
res.pca <- PCA(raw_data,quali.sup = c(1,5,6,7,9),quanti.sup = 10,ncp=3)
res.hcpc <- HCPC(res.pca, nb.clust = -1)
res.pca <- PCA(raw_data,quali.sup = c(1,5,6,7,9),quanti.sup = 10,ncp=2)
res.hcpc <- HCPC(res.pca, nb.clust = -1)
```
```{r}
res.hcpc <- HCPC(res.pca, nb.clust = -1, graph = FALSE)

```

 
```{r}
res.hcpc <- HCPC(res.pca, nb.clust = -1, graph = FALSE)
res.hcpc
plot.HCPC(res.hcpc, choice = 'map', draw.tree = FALSE, title = '', select=c("12"))


res.pca <- PCA(raw_data,quali.sup = c(1,5,6,7,9),quanti.sup = 10,ncp=3)
res.hcpc <- HCPC(res.pca, nb.clust = -1, graph = FALSE)
res.hcpc

```

Characterization of each supplier

**14) The methodology that you have used to describe clusters can also be used to describe a categorical variable, for instance the supplier. Use the function catdes(data, num.var=1) and explain how this information can be useful for the company.**


```{r}
catdes(raw_data, num.var=1)
catdes(raw_data, num.var=5)

catdes(raw_data, num.var=6)


```


**15) To simultaneously take into account quantitative and categorical variables in the clustering you should use the HCPC function on the results of the FAMD ones. FAMD stands for Factorial Analysis of Mixed Data and is a PCA dedicated to mixed data. Explain what will be the impacts of such an analysis on the results?**

```{r}
res.famd <- FAMD (raw_data, ncp = 5, graph = TRUE, sup.var =  c(10), axes = c(1,2), row.w = NULL, tab.comp = NULL)

res.hcpc.famd <- HCPC(res.famd, nb.clust = -1
                , graph = TRUE)

plot.HCPC(res.hcpc.famd, choice = 'map', draw.tree = FALSE, select = c(1,10), title = '')

raw_data_clust <- res.hcpc.famd$data.clust

res.famd_2 <- FAMD (raw_data_clust, ncp = 6, graph = TRUE, sup.var =  c(1,5,7,10), axes = c(1,2), row.w = NULL, tab.comp = NULL)
res.famd_2$eig

coordinates <- as.data.frame(res.famd_2$ind$coord)
coordinates$Price <- raw_data_clust$Price
coordinates
```

16) Perform a model to predict the Price. Explain how the previous analysis can help you to interpret the results.

```{r}


fit <- lm(Price ~ coordinates$Dim.1 + coordinates$Dim.2 + coordinates$Dim.3 + coordinates$Dim.4 + coordinates$Dim.5 + coordinates$Dim.6,raw_data_clust)
p <- raw_data_clust[1,]

predict_price <- function (x) {
  fit <-  lm(Price ~ coordinates$Dim.1 + coordinates$Dim.2 + coordinates$Dim.3 + coordinates$Dim.4 + coordinates$Dim.5 + coordinates$Dim.6, raw_data_clust)
  p <- as.data.frame(predict.FAMD(res.famd_2,x)$coord) 
  p <- p %>% rename(Dim1 = 'Dim 1', Dim2 = 'Dim 2',Dim3 = 'Dim 3',Dim4 = 'Dim 4', Dim5 = 'Dim 5', Dim6 = 'Dim 6')
  return(predict(fit,p))
}


percentage_learning <- 0.80

learning_set <- sample_n(raw_data_clust, percentage_learning * dim(raw_data_clust)[1])
res.famd_learning <- FAMD (learning_set, ncp = 5, graph = TRUE, sup.var =  c(1,5,7,10), axes = c(1,2), row.w = NULL, tab.comp = NULL)
res.hcpc.famd_learning <- HCPC(res.famd_learning, nb.clust = -1
                , graph = TRUE)

res.famd_learning_2 <- FAMD (learning_set_clust, ncp = 6, graph = TRUE, sup.var =  c(1,5,7,10), axes = c(1,2), row.w = NULL, tab.comp = NULL)

learning_set_clust <- res.hcpc.famd_learning$data.clust
coordinates_learning <- as.data.frame(res.famd_learning_2$ind$coord)
coordinates_learning$Price <- learning_set_clust$Price
coordinates_learning

predict_new_price <- function (x) {
  fit <-  lm(Price ~ coordinates_learning$Dim.1 + coordinates_learning$Dim.2 + coordinates_learning$Dim.3 + coordinates_learning$Dim.4 + coordinates_learning$Dim.5 + coordinates_learning$Dim.6, learning_set_clust)
  p <- as.data.frame(predict.FAMD(res.famd_2,x)$coord) 
  p <- p %>% rename(Dim1 = 'Dim 1', Dim2 = 'Dim 2',Dim3 = 'Dim 3',Dim4 = 'Dim 4', Dim5 = 'Dim 5', Dim6 = 'Dim 6')
  return(predict(fit,p))
}


```


**17) If someone ask you why you did one global model and not one model per supplier, what is your answer?**

Below we count the amount of data points we have for each supplier. We have to consider the advantages and the pitfalls of building a local vs. global models.



```{r}
raw_data %>% group_by(Supplier) %>%  count()
```


**18) These data contained missing values. One representative in the compagny suggests either to put 0 in the missing cells or to impute with the median of the variables. Comment. For the categorical variables with missing values, it is decided to create a new category “missing”. Comment.**

```{r}

summary(res.hcpc.famd)

res.hcpc.famd$call$X$Dim.1


```

