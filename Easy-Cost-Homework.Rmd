---
title: "R Notebook"
output: html_notebook
---

**Libraries**

```{r}

library(FactoMineR)
library(tidyr)
library(dplyr)
library(tidyverse)
library(magrittr)
library(ggplot2)
library(ggpubr)
library(factoextra)
library(gridExtra)
library(moments)

```



**Screw Caps Data**
```{r}

raw_data <- read.table("ScrewCaps.csv",header=TRUE, sep=",", dec=".", row.names=1)
head(raw_data)
summary(raw_data)
```

**2) We start with univariate and bivariate descriptive statistics. Using appropriate plot(s) or summaries answer the following questions.**

*a) How is the distribution of the Price? Comment your plot with respect to the quartiles of the Price.*

From the quantile data, the summary statistics are given by: median, 1Q and 3Q  as 14.432, 11.864 and 19.04 respectively. 

The plots, the kurtosis and the skewness parameters suggest the price follows a bimodal distribution that is "skewed right". 
The major mode is around 14 and the antimode is around 29. Furthermore, 50% of the prices in the range 11.864 and 19.04. This is consistent with graph where the majority of the density is concentrated inside this range and a long right tail of prices outside. 

The boxplot supports this analyis and suggests the values in the tail are outliers. 

```{r}

price_density <- ggdensity(raw_data,x="Price",y = "..count..",
                        color="darkblue",
                        fill="lightblue",size=0.5, 
                        alpha=0.2, 
                        title = "Screw Cap Price Distribution", 
                        linetype = "solid", add = c("median"))+ font("title", size = 12,face="bold")
  
price_boxplot <- ggboxplot(raw_data$Price, width = 0.1, fill ="lightgray", outlier.colour = "darkblue", outlier.shape=4.2, ylab = "Price", xlab = "Screw Caps" , title = "Price Box Plot") + rotate() + font("title", size = 12,face="bold")

price_quantile <- quantile(raw_data$Price)

ggarrange(price_density, price_boxplot, ncol = 1, nrow = 2)

price_quantile
skewness(raw_data$Price)
kurtosis(raw_data$Price)


```


*b) Does the Price depend on the Length? weight?*

We examine Price vs. Length, log(Price) vs. log(Length); Price vs. weight, log(Price) vs. log(weight) and provide the summary for each. 

The plots suggest somewhat of a relationship between the variables, but observing the results of the F and T tests confirm this to a high degree of significance. 

```{r}
price_length <- ggplot(raw_data, aes(x=Length, y=Price)) + geom_point() + geom_smooth(method=lm, color="darkgreen")+ theme_minimal()
price_length_log <- ggplot(raw_data, aes(x=log(Length), y=log(Price))) + geom_point() + geom_smooth(method=lm, color="darkgreen")+ theme_minimal()
price_weight <- ggplot(raw_data, aes(x=weight, y=Price)) + geom_point() + geom_smooth(method=lm,color="red")+theme_minimal()
price_weight_log <- ggplot(raw_data, aes(x=log(weight), y=log(Price))) + geom_point() + geom_smooth(method=lm,color="red")+theme_minimal()

ggarrange(ggarrange(price_length, price_length_log, ncol = 2, nrow = 1), ggarrange(price_weight, price_weight_log, ncol = 2, nrow = 1), ncol = 1, nrow = 2)

summary(lm(formula = Price ~ Length, raw_data))
summary(lm(formula = log(Price) ~ log(Length), raw_data))
summary(lm(formula = Price ~ weight, raw_data))
summary(lm(formula = log(Price) ~ log(weight), raw_data))
```


*c) Does the Price depend on the Impermeability? Shape?*  

The plots below suggests there is dependency on Impermeability - the medians differ significantly. 

```{r}
impermability_plot_1 <- ggdotplot(raw_data,x="Impermeability",y="Price",color = "Impermeability", palette = "jco",binwidth = 1,legend="none")
shape_plot_1 <- ggdotplot(raw_data,x="Shape",y="Price",color = "Shape", palette = "npg",binwidth = 1,legend="none")
impermability_plot_2 <- ggboxplot(raw_data,x="Impermeability",y="Price",color = "Impermeability", palette = "jco",legend="none")
shape_plot_2 <- ggboxplot(raw_data,x="Shape",y="Price",color = "Shape", palette = "npg", legend = "none")

ggarrange(ggarrange(impermability_plot_1,impermability_plot_2,ncol = 2, nrow = 1),
           ggarrange(shape_plot_1,shape_plot_2,ncol = 2, nrow = 1),
           ncol = 1, nrow = 2)
```

*d) Which is the less expensive Supplier?*

The answer to this question depends on the definition of expensive. 

First, examine the following absolute metrics (this can be seen via the boxplot)
1) Absolute price - Supplier B cheapest (6.477451). However, Supplier B is also the supplier which has the highest absolute price (46.610372)
2) Average Price - Supplier C cheapest (14.88869)

Second, examine the following relative metrics:  
3) Average Price / Unit Length - Supplier A (1.505043)
4) Average Price / Unit weight - Supplier A (9.013902)
5) Average Price / Unit Diameter - Supplier A (11.95632)

The result above suggest Supplier A has the cheapest average price per unit of production.

The analysis however is not complete given we do not have a definition of cheapest price. Even the scatter and box plots below suggest suppliers may cater to specific product ranges. It also ignores the categorical data which could provide some insights into cheapest price for certain product features Furthermore, we have not performed  statistical tests to examine the significance of these differences. 

```{r}

supplier_plot_1 <- ggboxplot(raw_data,x="Supplier",y="Price",color = "Supplier", palette = c("darkblue","red","darkgreen"),legend="none") + rotate()
supplier_plot_2 <- ggscatter(raw_data,x="Length",y="Price",color = "Supplier", palette = c("darkblue","red","darkgreen"),xscale= "log10", yscale="log10")
supplier_plot_3 <- ggscatter(raw_data,x="weight",y="Price",color = "Supplier", palette = c("darkblue","red","darkgreen"),xscale= "log10", yscale="log10")
supplier_plot_4 <- ggscatter(raw_data,x="Diameter",y="Price",color = "Supplier", palette = c("darkblue","red","darkgreen"),xscale= "log10", yscale="log10")

supplier_statistics <- raw_data %>% group_by(Supplier) %>%  summarise( "Average Price" = mean(Price), "Average Length" = mean(Length),"Average weight" = mean(weight),"Average Diameter" = mean(Diameter),  "Average Price / Length" = mean(Price)/mean(Length), "Average Price / weight" = mean(Price)/mean(weight), "Average Price / Diameter" = mean(Price)/mean(Diameter))

supplier_plot_1
supplier_plot_2
supplier_plot_3
supplier_plot_4
head(supplier_statistics)

```



**3) One important point in explanatory data analysis consists in identifying potential outliers. Could you give points which are suspect regarding the Mature.Volume variable? Give the characteristics (other features) of the observations that seem suspsect**

There are four data points which seem suspect - they have the same characteristics for Diameter, weight, nb.of.pieces, Impermeability, Finishing, Raw.Material and Mature.Volume. They differ in their supplier, price and length. These suggest some error in collating the data (system error / default data). 

```{r}

Mature.Volume_plot <- gghistogram(raw_data,x="Mature.Volume",y="..count..", color = "darkblue", fill = "lightgrey") + theme_minimal()
Mature.Volume_plot
raw_data %>% filter (Mature.Volume > 6e+05 ) 

```


For the rest of the analysis, the 4 data points above are disregarded. 

```{r}
library(dplyr)
raw_data <- raw_data %>% filter (Mature.Volume < 6e+05 )
```


**4) Perform a PCA on the dataset ScrewCap, explain briefly what are the aims of a PCA and how categorical variables are handled?**

Principal components analysis (PCA) is a technique for taking high-dimensional data, and using the dependencies between the variables to represent it in a more tractable, lower-dimensional form, without losing too much information - we try capture the essence of high dimentional data in a low dimensional representation. The aim of PCA is to draw conclusions from the linear relationships between variables by detecting the principal dimensions of variability. This may be for compression, denoising, data completion, anomaly detection or for preprocessing before supervised learning (improve performance / regularization to reduce overfitting).

The categorical variables cannot be represented in the same way as the supplementary quantitative variables since it is not possible to calculate the correlation between a categorical variable and the principal components. The categorical variables here are handled as supplemetary variables on a purely illustrative basis  - they are not used to calculate the distance between inidividuals. We represent a categorical variable at the barycentre of all the individuals possessing that variable. A categorical variable on the PCA performed below can therefore be regarded as the mean individual obtained from the set of individuals who have it.

Given our ultimate goal here is to explore data prior to a multiple regression, it is advisable to choose the explanatory variables for the regression model as active variables for PCA, and to project the variable to be explained (the dependent variable) as a supplementary variable. This gives some idea of the relationships between explanatory variables and thus of the need to select explanatory variables. This also gives us an idea of the quality of the regression: if the dependent variable is appropriately projected, it will be a well-fitted model. Thus we select Price as a supplementary variable. 

The dataset in this exercise contains 6 supplementary variables:
- 1 quantitative variable (Price)
- 5 qualitative variables (Supplier, Shape, Impermeability and Finishing).  


```{r}

res.pca <- PCA(raw_data,quali.sup = c(1,5,6,7,9),quanti.sup = 10, graph = FALSE)

fviz_pca_ind(res.pca, col.ind="cos2", label=c("quali"), geom = "point", title = "Individual factor map (PCA)", habillage = "none") + scale_color_gradient2(low="lightblue", mid="blue", high="darkblue", midpoint=0.6) + theme_minimal() 
plot.PCA(res.pca,choix = c("ind"),invisible = c("ind"))+theme_minimal()
plot.PCA(res.pca,choix = c("var"))+theme_minimal()

```



**5) Compute the correlation matrix between the variables and comment it with respect to the correlation circle**

The first task is to center and standardize the variables. Then the correlation matrix is computed. All variable vectors are quite near to the boundary of the correlation circle on the variables plot - thus the variables are relatively well projected on the 2 dimensional subspace. We now turn our attention to correlations between variables. 

The correlations can be visualised through the angles between variables on the correlation matrix. This can be related to the correlation matrix:
- the Dimaeter, the Length and the weight expose very strong corrleation: the angle between them is close to 0, suggesting correlation close to 1
- the three variables above are at an angle sightly wider than a right angle to both nb.of.pieces and Mature.Volume in the cirlce which suggests 

```{r}
don <- as.matrix(raw_data[,-c(1,5,6,7,9,10)]) %>% scale()
don_correlation <- cor(don)
don_correlation
```

**On what kind of relationship PCA focuses? Is it a problem?**

 PCA focuses on the linear rela- tionships between variables. More complex links also exist, such as quadratic relationships, logarithmics, exponential functions, and so forth, but they are not studied in PCA. This may seem restrictive, but in practice many relation- ships can be considered linear, at least for an initial approximation.



**Comment the PCA outputs.**
*Comment the position of the categories Impermeability=type 2 and Raw.Material=PS.*
*Comment the percentage of inertia*

```{r}
barplot(res.pca$eig[,2], names.arg = 1:nrow(res.pca$eig))
drawn <- c("90", "89", "164", "161", "163", "131")
plot.PCA(res.pca, select = "cos2 5", axes = 1:2, choix = 'ind', invisible = 'quali', title = '')
wilks.p <- structure(c(1.39936324848005e-27, 1.02072912984402e-22, 2.01088132732067e-11, 
3.37174568997818e-07, 0.394546176405119), .Names = c("Impermeability", "Raw.Material", "Shape", "Supplier", "Finishing"))
wilks.p

sample = sample(rownames(res.pca$call$X), length(rownames(res.pca$call$X)))
res.pca$call$X = res.pca$call$X[sample,]
res.pca$ind$coord = res.pca$ind$coord[sample[!sample %in% rownames(res.pca$ind.sup$coord)],]
res.pca$ind.sup$coord = res.pca$ind.sup$coord[sample[sample %in% rownames(res.pca$ind.sup$coord)],]

drawn <-c("90", "89", "164", "161", "163", "131")
hab <-"Impermeability"
plotellipses(res.pca, axes = 1:2, invisible = 'quali', select = "cos2 5", keepvar = hab, title = '')

drawn <-c("Length", "Diameter", "weight", "nb.of.pieces", "Price")
plot.PCA(res.pca, select = "cos2 5", axes = 1:2, choix = 'var', title = '')

drawn <- c("Type 1", "Type 2", "Supplier A", "PS", "PP", "Shape 2", "Shape 3", "Shape 1", "Lacquering", "Supplier B", "Supplier C","Hot Printing")
plot.PCA(res.pca, select = "cos2 5", axes = 1:2, choix = 'ind', invisible = c('ind', 'ind.sup'), title = '')



res.pca <- PCA(raw_data,quali.sup = c(1,5,6,7,9),quanti.sup = 10, ncp=3) #ncp = 3
res.hcpc <- HCPC(res.pca, nb.clust = -1
                , graph = FALSE)

drawn <- c("90", "89", "164", "161", "163", "131")
plot.HCPC(res.hcpc, choice = 'map', draw.tree = FALSE, select = "cos2 5", title = '')
dimdesc(res.pca, axes = 1:1)
res.hcpc$desc.var

```

Fisher test
Variance


```{r}

fviz_nbclust(don, kmeans, method = "wss") + geom_vline(xintercept = 3, linetype = 2)

```


*Comments the results and describe precisely one cluster* -- Add Fisher Test

The cluster 1 is made of individuals sharing :
- high values for the variable Mature.Volume. 
- low values for the variables nb.of.pieces, Price, weight, Length and Diameter (variables are sorted from the weakest).

The cluster 2 is made of individuals sharing :
- high values for the variable nb.of.pieces. 
- low values for the variables Mature.Volume, Diameter, Length, weight and Price (variables are sorted from the weakest).

The cluster 3 is made of individuals such as 89, 90, 131, 161, 163 and 164. This group is characterized by :
- high values for the variables Length, Diameter, weight and Price (variables are sorted from the strongest).
- low values for the variables nb.of.pieces and Mature.Volume (variables are sorted from the weakest).


*If someone ask you why you have selected k components and not k + 1 or k − 1, what is your answer? (could you suggest a strategy to assess the stability of the approach? - are there many differences between the clustering obtained on k components or on the initial data)*

```{r}
res.pca <- PCA(raw_data,quali.sup = c(1,5,6,7,9),quanti.sup = 10,ncp=4)
res.hcpc <- HCPC(res.pca, nb.clust = -1)
res.pca <- PCA(raw_data,quali.sup = c(1,5,6,7,9),quanti.sup = 10,ncp=3)
res.hcpc <- HCPC(res.pca, nb.clust = -1)
res.pca <- PCA(raw_data,quali.sup = c(1,5,6,7,9),quanti.sup = 10,ncp=2)
res.hcpc <- HCPC(res.pca, nb.clust = -1)
```
```{r}
res.hcpc <- HCPC(res.pca, nb.clust = -1, graph = FALSE)

```

 
```{r}
res.hcpc <- HCPC(res.pca, nb.clust = -1, graph = FALSE)
res.hcpc
plot.HCPC(res.hcpc, choice = 'map', draw.tree = FALSE, title = '', select=c("12"))


res.pca <- PCA(raw_data,quali.sup = c(1,5,6,7,9),quanti.sup = 10,ncp=3)
res.hcpc <- HCPC(res.pca, nb.clust = -1, graph = FALSE)
res.hcpc

```

Characterization of each supplier


```{r}
catdes(raw_data, num.var=1)
catdes(raw_data, num.var=5)

catdes(raw_data, num.var=6)


```


```{r}
res.famd <- FAMD (raw_data, ncp = 5, graph = TRUE, sup.var =  c(1,5,7,10), axes = c(1,2), row.w = NULL, tab.comp = NULL)


res.hcpc.famd <- HCPC(res.famd, nb.clust = -1
                , graph = TRUE)

plot.HCPC(res.hcpc.famd, choice = 'map', draw.tree = FALSE, select = c(1,10), title = '')


```


```{r}

summary(res.hcpc.famd)

res.hcpc.famd$call$X$Dim.1

library(e1071)

svm.model <- svm(Price ~ ., data = raw_data[1:150, -c(1,5,7)], cost = 100, gamma = 1)
svm.pred <- predict(svm.model, raw_data[151:191,-c(1,5,7,10)])
View(svm.pred)
```

