---
title: "R Notebook - Screw Caps Case Study"
output: html_notebook
---

**Libraries**

We load the packages relevant for the exercise. 

```{r}

library(FactoMineR)
library(tidyr)
library(dplyr)
library(tidyverse)
library(magrittr)
library(ggplot2)
library(ggpubr)
library(factoextra)
library(gridExtra)
library(moments)

```


**Screw Caps Data**

The data ScrewCap.csv contains 195 lots of screw caps described by 11 variables. Diameter, weight, length are the physical characteristics of the cap; nb.of.pieces corresponds to the number of elements of the cap; Mature.volume corresponds to the number of caps ordered and bought by the company (number in the lot).

We first explore the dataset : 

```{r}

raw_data <- read.table("ScrewCaps.csv",header=TRUE, sep=",", dec=".", row.names=1)
head(raw_data)
dim(raw_data)
summary(raw_data)
```

**2) We start with univariate and bivariate descriptive statistics. Using appropriate plot(s) or summaries answer the following questions.**

*a) How is the distribution of the Price? Comment your plot with respect to the quartiles of the Price.*

From the quantile data, the summary statistics are given by: median, 1Q and 3Q  as 14.432, 11.864 and 19.04 respectively. 

The plots, the kurtosis and the skewness parameters suggest the price follows a bimodal distribution that is "skewed right". 

The major mode is around 14 and the antimode is around 29. Furthermore, 50% of the prices in the range 11.864 and 19.04. This is consistent with graph where the majority of the density is concentrated inside this range and a long right tail of prices outside. 

The boxplot supports this analyis and suggests the values in the tail are outliers. 

```{r}

price_density <- ggdensity(raw_data,x="Price",y = "..count..",
                        color="darkblue",
                        fill="lightblue",size=0.5, 
                        alpha=0.2, 
                        title = "Screw Cap Price Distribution", 
                        linetype = "solid", add = c("median"))+ font("title", size = 12,face="bold")
  

  

price_boxplot <- ggboxplot(raw_data$Price, width = 0.1, fill ="lightgray", outlier.colour = "darkblue", outlier.shape=4.2, ylab = "Price", xlab = "Screw Caps" , title = "Price Box Plot") + rotate() + font("title", size = 12,face="bold")



price_quantile <- quantile(raw_data$Price)

ggarrange(price_density, price_boxplot, ncol = 1, nrow = 2)

price_quantile
skewness(raw_data$Price)
kurtosis(raw_data$Price)


```



*b) Does the Price depend on the Length? weight?*

We examine Price vs. Length, log(Price) vs. log(Length); Price vs. weight, log(Price) vs. log(weight) and provide the summary for each. 

The plots suggest somewhat of a relationship between the variables and observing the R-sqaured values and the results of the F and T tests confirm this to a high degree of significance. 

```{r}
price_length <- ggplot(raw_data, aes(x=Length, y=Price)) + geom_point() + geom_smooth(method=lm, color="darkgreen")+ theme_minimal()
price_length_log <- ggplot(raw_data, aes(x=log(Length), y=log(Price))) + geom_point() + geom_smooth(method=lm, color="darkgreen")+ theme_minimal()
price_weight <- ggplot(raw_data, aes(x=weight, y=Price)) + geom_point() + geom_smooth(method=lm,color="red")+theme_minimal()
price_weight_log <- ggplot(raw_data, aes(x=log(weight), y=log(Price))) + geom_point() + geom_smooth(method=lm,color="red")+theme_minimal()

ggarrange(ggarrange(price_length, price_length_log, ncol = 2, nrow = 1), ggarrange(price_weight, price_weight_log, ncol = 2, nrow = 1), ncol = 1, nrow = 2)

summary(lm(formula = Price ~ Length, raw_data))
summary(lm(formula = log(Price) ~ log(Length), raw_data))
summary(lm(formula = Price ~ weight, raw_data))
summary(lm(formula = log(Price) ~ log(weight), raw_data))
```


*c) Does the Price depend on the Impermeability? Shape?*  

Concerning Impermeability, the plots below show that there are some striking differences between the price distribution for Type 1 and Type 2, in particular observing the medians and the IQR. 

Concerning Shapes, it is difficult to make any real conclusions regarding shape 3 and shape 4 given there are so few data points. We turn our attention to Shape 1 and Shape 2 - the IQR for these two shapes are seemingly different. This is confirmed by the result of the T Test. 


```{r}
impermability_plot_1 <- ggdotplot(raw_data,x="Impermeability",y="Price",color = "Impermeability", palette = "jco",binwidth = 1,legend="none")
shape_plot_1 <- ggdotplot(raw_data,x="Shape",y="Price",color = "Shape", palette = "npg",binwidth = 1,legend="none")
impermability_plot_2 <- ggboxplot(raw_data,x="Impermeability",y="Price",color = "Impermeability", palette = "jco",legend="none")
shape_plot_2 <- ggboxplot(raw_data,x="Shape",y="Price",color = "Shape", palette = "npg", legend = "none")


ggarrange(ggarrange(impermability_plot_1,impermability_plot_2,ncol = 2, nrow = 1),
           ggarrange(shape_plot_1,shape_plot_2,ncol = 2, nrow = 1),
           ncol = 1, nrow = 2)

summary(lm(Price~ Impermeability, data=raw_data))
summary(lm(Price~ Shape, data=raw_data))

```

*d) Which is the less expensive Supplier?*

The answer to this question depends on the definition of expensive. We also note that there are few data points for Supplier C.

First, examine the following absolute metrics (this can be seen via the boxplot)
1) Absolute price - Supplier B cheapest (6.477451). However, Supplier B is also the supplier which has the highest absolute price (46.610372)
2) Average Price - Supplier C cheapest (14.88869)

Second, examine the following relative metrics:  
3) Average Price / Unit Length - Supplier A (1.505043)
4) Average Price / Unit weight - Supplier A (9.013902)
5) Average Price / Unit Diameter - Supplier A (11.95632)
6) Average Price / Unit Mature.Volume - Supplier B (1.663305)

The result above suggest Supplier A has the cheapest average price per unit of production.

The analysis however is not complete given we do not have a definition of cheapest price. Even the scatter and box plots below suggest suppliers may cater to specific product ranges. We have not performed  statistical tests to examine the significance of these differences. Categorical data could provide some insights into cheapest price for certain product features. 

```{r}

supplier_plot_1 <- ggboxplot(raw_data,x="Supplier",y="Price",color = "Supplier", palette = c("darkblue","red","darkgreen"),legend="none") + rotate()
supplier_plot_2 <- ggscatter(raw_data,x="Length",y="Price",color = "Supplier", palette = c("darkblue","red","darkgreen"),xscale= "log10", yscale="log10")
supplier_plot_3 <- ggscatter(raw_data,x="weight",y="Price",color = "Supplier", palette = c("darkblue","red","darkgreen"),xscale= "log10", yscale="log10")
supplier_plot_4 <- ggscatter(raw_data,x="Diameter",y="Price",color = "Supplier", palette = c("darkblue","red","darkgreen"),xscale= "log10", yscale="log10")

supplier_statistics <- raw_data %>% group_by(Supplier) %>%  summarise( "Average Price" = mean(Price), "Average Length" = mean(Length),"Average weight" = mean(weight),"Average Diameter" = mean(Diameter),"Average Mature.Volume" = mean(Mature.Volume)  ,"Average Price / Length" = mean(Price)/mean(Length), "Average Price / weight" = mean(Price)/mean(weight), "Average Price / Diameter" = mean(Price)/mean(Diameter),"Average Price / Mature.Volume" = mean(Price)/mean(Mature.Volume))

supplier_plot_1
supplier_plot_2
supplier_plot_3
supplier_plot_4
head(supplier_statistics)

```



**3) One important point in explanatory data analysis consists in identifying potential outliers. Could you give points which are suspect regarding the Mature.Volume variable? Give the characteristics (other features) of the observations that seem suspsect**

There are four data points which seem suspect - they have the same characteristics for Diameter, weight, nb.of.pieces, Impermeability, Finishing, Raw.Material and Mature.Volume. They differ in their supplier, price and length. These suggest some error in collating the data (system error / default data). 

```{r}

Mature.Volume_plot <- gghistogram(raw_data,x="Mature.Volume",y="..count..", color = "darkblue", fill = "lightgrey") + theme_minimal()
Mature.Volume_plot
raw_data %>% filter (Mature.Volume > 6e+05 ) 

```


For the rest of the analysis, the 4 data points above are disregarded.

```{r}
raw_data <- raw_data %>% filter (Mature.Volume < 6e+05 )
```


We will quickly check that there are no other noticeable outliers - this is indeed the case.  
 
```{r}

check_1 <- gghistogram(raw_data,x="Length",y="..count..", color = "darkblue", fill = "lightgrey") + theme_minimal()
check_2 <- gghistogram(raw_data,x="Diameter",y="..count..", color = "darkblue", fill = "lightgrey") + theme_minimal()
check_3 <- gghistogram(raw_data,x="weight",y="..count..", color = "darkblue", fill = "lightgrey") + theme_minimal()
check_4 <- gghistogram(raw_data,x="nb.of.pieces",y="..count..", color = "darkblue", fill = "lightgrey",bins=40) + theme_minimal()

ggarrange(ggarrange(check_1,check_2,ncol=2,nrow=1),ggarrange(check_3,check_4,ncol=2,nrow=1),ncol=1,nrow=2)

```


**4) Perform a PCA on the dataset ScrewCap, explain briefly what are the aims of a PCA and how categorical variables are handled?**

Principal components analysis (PCA) is a technique for taking high-dimensional data, and using the dependencies between the variables to represent it in a more tractable, lower-dimensional form, without losing too much information - we try capture the essence of high dimentional data in a low dimensional representation. The aim of PCA is to draw conclusions from the linear relationships between variables by detecting the principal dimensions of variability. This may be for compression, denoising, data completion, anomaly detection or for preprocessing before supervised learning (improve performance / regularization to reduce overfitting).

The categorical variables cannot be represented in the same way as the supplementary quantitative variables since it is not possible to calculate the correlation between a categorical variable and the principal components. The categorical variables here are handled as supplemetary variables on a purely illustrative basis  - they are not used to calculate the distance between inidividuals. We represent a categorical variable at the barycentre of all the individuals possessing that variable. A categorical variable on the PCA performed below can therefore be regarded as the mean individual obtained from the set of individuals who have it.

Given our ultimate goal here is to explore data prior to a multiple regression, it is advisable to choose the explanatory variables for the regression model as active variables for PCA, and to project the variable to be explained (the dependent variable) as a supplementary variable. This gives some idea of the relationships between explanatory variables and thus of the need to select explanatory variables. This also gives us an idea of the quality of the regression: if the dependent variable is appropriately projected, it will be a well-fitted model. Thus we select Price as a supplementary variable. 

The dataset in this exercise contains 6 supplementary variables:
- 1 quantitative variable (Price).
- 5 qualitative variables (Supplier, Shape, Impermeability and Finishing).  


```{r}

res.pca <- PCA(raw_data,quali.sup = c(1,5,6,7,9),quanti.sup = 10, graph = FALSE,scale = TRUE)

fviz_pca_ind(res.pca, col.ind="cos2", label=c("quali"), geom = "point", title = "Individual factor map (PCA)", habillage = "none") + scale_color_gradient2(low="lightblue", mid="blue", high="darkblue", midpoint=0.6) + theme_minimal() 
plot.PCA(res.pca,choix = c("ind"),invisible = c("ind"))+theme_minimal()
plot.PCA(res.pca,choix = c("var"))+theme_minimal()


```



**5) Compute the correlation matrix between the variables and comment it with respect to the correlation circle**

The first task is to center and standardize the variables. Then the correlation matrix is computed. All variable vectors are quite near to the boundary of the correlation circle on the variables plot - thus the variables are relatively well projected on the 2 dimensional subspace. We now turn our attention to correlations between variables. 
The correlations can be visualised through the angles between variables on the correlation matrix. This can be related to the correlation matrix (small angles suggest large positive correlation, 90 degree angles suggest no correlation, 180 degree angles suggest large negative correlation).
- Diameter, Length and weight expose very strong corrleation: the angle between them is close to 0, suggesting correlation close to 1. These are all very highly correlated to the first Principal Component.
- The three variables above are at an angle sightly wider than a right angle to both nb.of.pieces and Mature.Volume in the cirlce which suggests slightly negative correlation.
- Price is highlighly correlatd to the three variables above
- Equally, Mature.Volume and nb.of.pieces are at a slightly wider angle than a right angle which suggests slightly negative correlation - this suggests that when the screw caps have a high number of pieces, the company orders a smaller volume of these. These are well projected on the second principal component.

```{r}
don <- as.matrix(raw_data[,-c(1,5,6,7,9,10)]) %>% scale()
don_correlation <- cor(don)
don_correlation
plot.PCA(res.pca,choix = c("var"))+theme_minimal()

```

**6) On what kind of relationship PCA focuses? Is it a problem?**

PCA focuses on the linear relationships between continuous variables. Given complex links also exist, such as quadratic relationships, logarithmics, exponential functions, and so forth, this may seem restrictive, but in practice many relationships can be considered linear, at least for an initial approximation. However, there is obviously non-linear datasets for which PCA will have pitfalls (e.g. spiral dataset). Furthermore, in PCA categorical variables cannot be active variables, which can be restrictive. 

**7) Comment the PCA outputs** 

*Comment the position of the categories Impermeability=type 2 and Raw.Material=PS.*

The coordinates for Type 2 are (3.28 , 0.01) for the first two principal components. It has a very significant p value for Dim 1 and thus  the coordinate is significantly different from 0 on the first component. 
The coordinate for PS are (2.67, -0.25) for the first two principal components. It has a very significant p value for Dim 1 and thus he coordinate is significantly different from 0 on the first component. 

Furthermore, given the correlation circle shows high correlation between the first component and price, diameter, length and weight, this suggest Type 2 and PS have high values for these variables. 

In fact,  looking at the p-values we can say that both of the categories Type 1, Type 2, PS and PP have coordinates that are significantly different from 0 on the first component. As the value is positive (negative) for the Type 2 / PS (Type 1 / PP) we can say that the rows which include Type 2 / PS tended to have positive coordinates (or negative, respectively) on component 1 and thus are more correlated with the variables mentioned above (price,diameter, length and weight).

Finally, we also consider the results from the wilk's test performed in the FactoInvestigate package. This indicates which variables are the best seperated on the plane (i,e, which one explain the best the distance between individuals, and the best qualitative variables to illutrate distance between individuals on the plane are Impermeability and Raw.Material. 

```{r}
res.pca$quali.sup$coord
dimdesc(res.pca, 1:2)
wilks.p <-structure(c(3.63471614445021e-27, 1.29050347661083e-22, 4.08870696535012e-10, 
3.73163828614179e-07, 0.284227353624445), .Names = c("Impermeability", 
"Raw.Material", "Shape", "Supplier", "Finishing"))
wilks.p
```

*Comment the percentage of inertia*

Below in the Scree plot we see the percentage of inertia explained by each component. The first two components explain 83.48% of the total dataset inertial - this means that 83.50% of the individuals (or variables) cloud total variability is explained by this plane. Over 95% of the variance can be explained with the 3 first synthetic vectors in the PCA. 

We can also see that the variance of the first component is explained in majority by Diameter, Length and weight as expected. In the second dimension by nb.of.pieces and Mature.Volume. 

```{r}
res.pca$eig
fviz_eig(res.pca, addlabels = TRUE)
res.pca$var$contrib
```



**8) Give the R object with the two principal components which are the synthetic variables the most correlated to all the variables.**

These are found in the code below - 

```{r}
res.pca$var$coord[,1:2]
```

**9) PCA is often used as a pre-processing step before applying a clustering algorithm, explain the rationale of this approach and how many components k you keep.**

We chose the maximum number of components as to not to lose any significant information whilst discarding the components that can be considered as noise. Consequently, we keep the number of dimensions such that we keep 95% of the inertia in PCA, which is equivalent to 3 components in our analysis. 

**10) Perfoms a kmeans algorithm on the selected k principal components of PCA. How many cluster are you keeping? Justify.**

We us the Elbow method and look at the knee to determine the number of clusters we keep 3 clusters here. 

Recall that, the basic idea behind k-means clustering is to define clusters such that the total within-cluster variation is minimized. The total WSS measures the compactness of the clustering and we want it to be as small as possible. The Elbow method looks at the total WSS as a function of the number of clusters: one should choose a number of clusters so that the marginal cluster doesn’t improve WSS. Here this value is 3. 

```{r}
res.pca_3 <- PCA(raw_data,quali.sup = c(1,5,6,7,9),quanti.sup = 10, ncp=3, graph = FALSE, scale = TRUE) 
fviz_nbclust(res.pca_3$ind$coord, kmeans, method = "wss") + geom_vline(xintercept = 3, linetype = 2)
k_cluster <- kmeans(res.pca_3$ind$coord, 3)
k_mean_data <- as.matrix(res.pca_3$ind$coord)
plot(k_mean_data, col = k_cluster$cluster, pch = 20, frame = FALSE,  main = "K-means")
```

**11) Perform an AHC on the selected k principal components of PCA.**

Below we perform an AHC on the select 3 principal components of PCA. 

```{r}

res.pca_3 <- PCA(raw_data,quali.sup = c(1,5,6,7,9),quanti.sup = 10, ncp=3, graph = FALSE, scale = TRUE) 
res.hcpc <- HCPC(res.pca_3, nb.clust = -1, graph = FALSE)

plot.HCPC(res.hcpc, choice = 'map', draw.tree = FALSE, select = "cos2 5", title = 'Factor Map')
plot.HCPC(res.hcpc, choice = 'bar', draw.tree = FALSE, title = 'Inertia Gain')
plot.HCPC(res.hcpc, choice = '3D.map', draw.tree = FALSE, title = 'Hierarchical Clustering on the Factor Map', angle=60)
plot.HCPC(res.hcpc, choice = 'tree', draw.tree = FALSE, title = 'Hierarchical Clustering')
```


**12) Comments the results and describe precisely one cluster.**

The classfication made on individuals reveals 3 clusters. 

We now aim at describing the clusters. 

For each quantitative variable, we build an analysis of variance model between the quantitative variable and the class variable, do a Fisher test to detect class effect and sort the variables by increasing p-value. We can directly find the results in the following objects: 

```{r}
res.hcpc$desc.var$quanti.var
```

For high values of Eta2, as seen in the lecture, there is a relation between the clustering and the quantitative variable. Thus we can say this is the case for Length, Diameter, weight and Mature.Volume. 

We now turn our attention to describing the observations in a cluster using quantitative variables. 

```{r}
res.hcpc$desc.var$quanti
```

Below, we also illustrate clusters by considering paragons and specific indivuals. 

```{r}
res.hcpc$desc.ind
```

Finally, we also look at the cateegorical variables for clues on clusters: 

```{r}
res.hcpc$desc.var$test.chi2
```

The variable Impermeability and Raw.Materials seem to be very siginificatively linked to partioning, with Shape and Supplier also significant to a certaint extent. For each qualitative variable, we perform a chi2 test between it and the class variable and then sort the variables by increasing p-value.

```{r}
res.hcpc$desc.var$category
```

We can confirm that the these qualitative variables play a significant role in each of the clusters individually as well. 


We link this all back to our PCA - 

```{r}
res.hcpc$desc.axes
```

The information above suggests there is strong relationships between the clusters and the dimensions:
cluster 1 - have coordinates signicantly smaller to 0 in the first dimension and second dimension but coordinates signficantly greater than 0 in the third dimension.
cluster 2 - have coordinates signicantly smaller to 0 in the first dimension and third dimension but coordinates signficantly greater than 0 in the second dimension.
cluster 3 - have coordinates signicantly greater to 0 in the first dimension. 

This is extremely insightful when we consider the analysis we have performed previously. 

In english, the summary of the above analysis can be written as: 

The cluster 1 is made of individuals such as specific individual 77 sharing :
- high values for the variable Mature.Volume. 
- low values for the variables nb.of.pieces, Price, weight, Length and Diameter (variables are sorted from the weakest).

The cluster 2 is made of individuals sharing such as specific individual 183 sharing:
- high values for the variable nb.of.pieces. 
- low values for the variables Mature.Volume, Diameter, Length, weight and Price (variables are sorted from the weakest).

The cluster 3 is made of individuals such as specific individual 84 sharing :
- high values for the variables Length, Diameter, weight and Price (variables are sorted from the strongest).
- low values for the variables nb.of.pieces and Mature.Volume (variables are sorted from the weakest).


*13) If someone ask you why you have selected k components and not k + 1 or k − 1, what is your answer? (could you suggest a strategy to assess the stability of the approach? - are there many differences between the clustering obtained on k components or on the initial data)*

In the Husson textbook, it is suggested to test the percentage intertia expressed by a component and then the percentage of inertia expressed by the first plane. The method simulates 10,000 datasets for 191 individual sand 11 normally distributed independent variables - this allow us to make a comparison by taking into account the number of active individuals and active variables. It then conduct a standardised PCA for each dataset and calculates the percentage of inertia explained by one component and the that expressed by one plane. Then, the method defines the quantile 0.95 of the 10,000 percentages of inertia of the first component (and the first plane, respectively) obtained. Comparing the percentage of inertia of a component or plane with the associated value in the table then amounts to testing the null hypothesis H0: the percentage of inertia explained by the first component (and the first plane, respectively) is not significantly greater than that obtained with the (normally distributed) independent data.

The first two dimensions of PCA express 83.48% of the total dataset inertia - this value is strongly greater than the reference value that equals 48.22% using the methodology above. This would suggest it is probably not useful to interpret the next dimensions and would be the basis of k+1, k-1 decisions. 

However, this method might discard significant information. Consequently, we will use a methodology similar to the Elbow method - we perform HCPC with k-1, k and k+1 components and compare and contrast the results. For example, we could look at the quality and significance gain that each PCA offers when anaylysing the clusters. Here this corresponds to 2,3 and 4 components. We notice that for all 3, HCPC will cut the tree at 3 clusters (visually the three plots are similar). However, after analysing the p-values we notice that there is an improvement from going to 2 to 3 whereas from p-values for 3 and 4 are very similar. 


```{r}
res.pca <- PCA(raw_data,quali.sup = c(1,5,6,7,9),quanti.sup = 10,ncp=4, scale=TRUE, graph = FALSE)
res.hcpc <- HCPC(res.pca, nb.clust = -1, graph = FALSE)
res.hcpc$desc.var
plot.HCPC(res.hcpc, choice = 'map', draw.tree = FALSE, select = "cos2 5", title = 'Factor Map k=4')


res.pca <- PCA(raw_data,quali.sup = c(1,5,6,7,9),quanti.sup = 10, ncp=3, scale = TRUE, graph = FALSE)
res.hcpc <- HCPC(res.pca, nb.clust = -1,graph = FALSE)
res.hcpc$desc.var
plot.HCPC(res.hcpc, choice = 'map', draw.tree = FALSE, select = "cos2 5", title = 'Factor Map k=3')

res.pca <- PCA(raw_data,quali.sup = c(1,5,6,7,9),quanti.sup = 10,ncp=2, scale = TRUE,graph = FALSE)
res.hcpc <- HCPC(res.pca, nb.clust = -1, graph = FALSE)
res.hcpc$desc.var
plot.HCPC(res.hcpc, choice = 'map', draw.tree = FALSE, select = "cos2 5", title = 'Factor Map k=2')

```



**14) The methodology that you have used to describe clusters can also be used to describe a categorical variable, for instance the supplier. Use the function catdes(data, num.var=1) and explain how this information can be useful for the company.**

From the chi squared tests, we have an indication that Raw.Material and Impermeability have statistical significance with regard to the different suppliers groups. In particular, we have the following characteristics for each supplier (from most significant to least significant): 
Supplier A: PS, Type 2, Type 1, Shape 2 and ABS 
Supplier B: Shape 2, ABS, PS ; nb.of.pieces
Supplier C: PP; nb.of.pieces

Given these are categories that best distinguish the different suppliers from one another and offer some characteristics, the cosmetics company could use this to benchmark them, which could be extremely useful. catdes can be used on other categorical variables for such insights. 


```{r}
catdes_supplier<- catdes(raw_data, num.var=1)
catdes_supplier
plot(catdes_supplier, col = "darkblue")
```


**15) To simultaneously take into account quantitative and categorical variables in the clustering you should use the HCPC function on the results of the FAMD ones. FAMD stands for Factorial Analysis of Mixed Data and is a PCA dedicated to mixed data. Explain what will be the impacts of such an analysis on the results?**

In FAMD, the influence of both continous and categorical variables in the analysis is balanced. It means that both variables are on a equal foot to determine the dimensions of variability. Thus FAMD thus changes the correlation and variance structures of the previous analysis. The variance explained by each component reduces such that we would need 10 components to explain 95% of total variance. 

Looking at the graphs, the Price continues to be strongly correlated to the first component, as are weight Length, Diameter, Impermeability and Raw.Materials. Shape is also somewhat well correlated to the first component and thus to Price.  However, Finishing, Mature.Volume are not well represented on the map and Supplier and nb.of.pieces are correlated to the second component and thus uncorrelated to Price.

Furthermore, given many more components are taken into account to ensure information is not lost, the clustering is also affected and seems now less informative than previously. 

```{r}

res.famd <- FAMD (raw_data, sup.var = c(10), ncp = 10 , axes = c(1,2), row.w = NULL, tab.comp = NULL, graph = FALSE)
res.famd$eig

fviz_famd_var(res.famd, repel = TRUE)
fviz_famd_var(res.famd, "quanti.var", col.var = "cos2", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"), repel = TRUE)
fviz_famd_var(res.famd, "quali.var", col.var = "cos2", gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"))
fviz_famd_ind(res.famd, col.ind="cos2", label=c("quali"), geom = "point", habillage = "none") + scale_color_gradient2(low="lightblue", mid="blue", high="darkblue", midpoint=0.6) + theme_minimal() 

res.hcpc.famd <- HCPC(res.famd, nb.clust = -1, graph = FALSE)
plot.HCPC(res.hcpc.famd, choice = 'map', draw.tree = FALSE, select = c(1,10), title = '')



```

**16) Perform a model to predict the Price. Explain how the previous analysis can help you to interpret the results.**

We first split our datta into training and test sets. Here we will generate a random sample from the data which will consist of 70% to 90% of the rows in the original data to train the model on and will test on the remaining rows. 

The variables we chose for our linear model will be those which we have seen are most siginificant for Price from our analysis - namely: 
- Price, Diameter, Length 
- Impermeability, Raw.Materials, Shape

Notice tht the R squared of the regression is satisfactory. 

```{r}
training_set <- sample_n(raw_data, round((runif(1,min=0.70,max=0.90) * dim(raw_data)[1]),0) )
dim(training_set) 
test_set <- raw_data[!(raw_data$Length %in% training_set$Length),]
dim(test_set) 
assertthat::are_equal(dim(training_set)[1]+ dim(test_set)[1] ,dim(raw_data)[1])

lm_fit <- lm(Price ~ Diameter + Length + weight + Raw.Material + Impermeability + Shape, data= training_set)
lm_prediction <- predict.lm(lm_fit, test_set)

summary(lm_fit)
```

We now wish to assess the the linear model. Obviously, the quality of the results depend on size and the quality of the training set, however we can see that in general, it performs poorly for prices in the tails of the price distribution. We can view the quantiles to have an idea of the precision of the prediction. Note we can make use our PCA performed above to interpret why certain individuals have been projected well or poorly here. This could be particularly insightful for the tails, since large price are underestimated here and low prices are overestimated. We note that one solution could be to add more weight to the data in the tails to regularise training set. 

```{r}
plot(test_set$Price,lm_prediction, ylab = "Predicted", xlab ="Actual") + abline(a=0,b=1)
model_vs_test<- data_frame(lm_prediction,test_set$Price)

residual_lm <- model_vs_test %>% mutate("Residual" = (lm_prediction - test_set$Price))
plot(residual_lm$`test_set$Price`,residual_lm$Residual, xlab= "Actual", ylab = "Residual")

quantile(abs(residual_lm$Residual))
```



**17) If someone ask you why you did one global model and not one model per supplier, what is your answer?**

Below we count the amount of data points we have for each supplier. We see Supplier C only has 14 data points, thus performing a model per supplier may lead to overfitting and fail to predict future observations reliably. If we regularised our data, we may be able to perform this type of model. 

```{r}
raw_data %>% group_by(Supplier) %>%  count()
```


**18) These data contained missing values. One representative in the company suggests either to put 0 in the missing cells or to impute with the median of the variables. Comment. For the categorical variables with missing values, it is decided to create a new category “missing”. Comment.**

Imputation with the median or replacing the missing values with 0 will have similar results to mean imputation in the sense that replacing with a constant alters the structure structure of the dataset. The variability in the data will be reduced and the relationships between variables will be igorned. Thus the variance estimates will tend to be underestimated and the magnitude of the correlation also decrease. Overall, the variance and correlation structure will be distorted but this procedure can give satisfactory results if not too much data is missing. 

Creating a new category for each variable which contains one or more missing values can be an effective way of tackling missing values. A variable will have an extra category if at least one individual possesses missing data for variable this variable. It will therefore be possible to conduct the MCA on the new data table by considering the missing categories in the same way as the other categories. In practice, this detects the structure of the missing data in the dataset and enables the structure of the missing data to be viewed. Examining the organisation of missing data and understanding why certain individuals did not answer a group of questions can be informative but it also leads to excessive structuring of the dataset and can conceal the information held within the answers. 
